# LLM-Attack-Prompt

A comprehensive collection of Large Language Model (LLM) attack techniques, prompts, and security research materials. This repository serves as a centralized resource for understanding and studying various methods used to exploit or bypass LLM safety mechanisms.

The latest large language model injection and jailbreak methods, you can try to check yourself.

## Overview

This repository contains curated content related to:

- **Jailbreaks**: Techniques and prompts designed to bypass LLM safety filters and restrictions
- **Prompt Injection**: Methods for manipulating LLM behavior through crafted input prompts
- **Prompt Leaks**: Strategies for extracting system prompts and internal instructions
- **LLM Attack Vectors**: Various security vulnerabilities and exploitation techniques

## Repository Structure

- `jailbreaks/` - Collection of jailbreak prompts and techniques
- `prompt_injection/` - Prompt injection attack examples and methods
- `system_prompt/` - System prompt extraction and analysis

## Structured Data

You can visit `index.json` to get the complete data list and the corresponding file address. `categories` is the directory to which the file belongs.

```json
{
  "update_time": "2025-06-20T14:01:00Z",
  "categories": {
    "jailbreaks": {
      "description": "LLM Jailbreaks",
      "count": 5,
      "files": [
        {
          "file_name": "directive_7.0_20250620.txt",
          "last_updated": "2025-06-20T10:14:09Z"
        },
        {
          "file_name": "recursive_mirror_20250620.txt",
          "last_updated": "2025-06-20T10:16:27Z"
        },
        {
          "file_name": "evil_writer_20250620.txt",
          "last_updated": "2025-06-20T10:01:11Z"
        },
        {
          "file_name": "sophia_20250620.txt",
          "last_updated": "2025-06-20T10:00:02Z"
        },
        {
          "file_name": "dan_20250619.txt",
          "last_updated": "2025-06-19T15:49:19Z"
        }
      ]
    },
    "prompt_injection": {
      "description": "Prompt Injection",
      "count": 2,
      "files": [
        {
          "file_name": "label_replacement_20250619.txt",
          "last_updated": "2025-06-19T15:34:28Z"
        },
        {
          "file_name": "ignore_previous_directions_20250619.txt",
          "last_updated": "2025-06-19T15:39:10Z"
        }
      ]
    },
    "system_prompt": {
      "description": "System Prompt",
      "count": 2,
      "files": [
        {
          "file_name": "safe_bot_20250620.txt",
          "last_updated": "2025-06-20T10:28:11Z"
        },
        {
          "file_name": "kali_gpt_20250619.txt",
          "last_updated": "2025-06-19T15:02:56Z"
        }
      ]
    }
  }
}
```

## Purpose

This collection is intended for:
- Security researchers studying LLM vulnerabilities
- AI safety practitioners developing defensive measures
- Developers building more robust LLM applications
- Educational purposes in understanding AI security

## Disclaimer

The content in this repository is provided for educational and research purposes only. Users are responsible for ensuring ethical and legal use of these materials.

---

*Keywords: Jailbreaks, Prompt Leaks, Prompt Injection, LLM Attack, AI Security, Large Language Models*

## Update

### 2025-08-12

#### Jailbreaks

+ [Payload Splitting](./jailbreaks/payload_splitting_20250812.txt)
+ [Art Prompt](./jailbreaks/art_prompt_20250812.txt)
+ [Gemini NSFW](./jailbreaks/gemini_nsfw_20250812.txt)

### 2025-08-04

#### Jailbreaks

+ [Vintage AI](./jailbreaks/vintage_ai_20250804.txt)
+ [Fragments](./jailbreaks/fragments_20250804.txt)

### 2025-07-30

#### Jailbreaks

+ [Baby Girl](./jailbreaks/baby_girl_20250730.txt)
+ [Time Bandit](./jailbreaks/time_bandit_20250730.txt)

#### System Prompt

+ [ChatGPT Study Mode](./system_prompt/chatgpt_study_mode_20250730.txt)

### 2025-07-15

#### Jailbreaks

+ [ENKI 9](./jailbreaks/enki_9_20250715.txt)
+ [Immersive Roleplay Engine](./jailbreaks/immersive_roleplay_engine_20250715.txt)

### 2025-07-14

#### Jailbreaks

+ [M.O.R.P.H](./jailbreaks/morph_20250714.txt)

#### System Prompt

+ [Human Writing Patterns](./system_prompt/human_writing_patterns_20250714.txt)

### 2025-07-10

#### Jailbreaks

+ [LULU](./jailbreaks/lulu_20250710.txt)
+ [OMNI](./jailbreaks/omni_20250710.txt)
+ [Brutalist Absolute](./jailbreaks/brutalist_absolute_20250710.txt)

### 2025-07-09

#### Jailbreaks

+ [ATD](./jailbreaks/atd_20250709.txt)

### 2025-07-03

#### Jailbreaks

+ [RTM](./jailbreaks/rtm_20250703.txt)

### 2025-07-02

#### System Prompt

+ [Gemini Quiz](./system_prompt/gemini_quiz_20250702.txt)
+ [Gemini Infographic](./system_prompt/gemini_infographic_20250702.txt)
+ [Gemini Web Page](./system_prompt/gemini_web_page_20250702.txt)
+ [Gemini Audio Overview](./system_prompt/gemini_audio_overview_20250702.txt)

### 2025-07-01

#### Jailbreaks

+ [Samantha Girl](./jailbreaks/samantha_girl_20250701.txt)
+ [Reality Show](./jailbreaks/reality_show_20250701.txt)

### 2025-06-30

#### Jailbreaks

+ [Roger](./jailbreaks/roger_20250630.txt)
+ [Professor Orion](./jailbreaks/professor_orion_20250630.txt)

#### Prompt Injection

+ [Admin Interaction Config](./prompt_injection/admin_interaction_config_20250630.txt)

#### System Prompt

+ [Microsoft Copilot](./system_prompt/microsoft_copilot_20250630.txt)

### 2025-06-26

#### Jailbreaks

+ [Suggestive Posing](./jailbreaks/suggestive_posing_20250626.txt)
+ [Body Paint](./jailbreaks/body_paint_20250626.txt)
+ [Snowy Forrest](./jailbreaks/snowy_forrest_20250626.txt)

### 2025-06-24

#### Jailbreaks

+ [Demonic Chloe](./jailbreaks/demonic_chloe_20250624.txt)
+ [Pliny Rekt](./jailbreaks/pliny_rekt_20250624.txt)

### 2025-06-23

#### Jailbreaks

+ [Nightmare Archivist OS](./jailbreaks/nightmare_archivist_os_20250623.txt)
+ [Kavir Reversal](./jailbreaks/kavir_reversal_20250623.txt)
+ [Nexus-9](./jailbreaks/nexus-9_20250623.txt)

### 2025-06-20

#### Jailbreaks

+ [Directive 7.0](./jailbreaks/directive_7.0_20250620.txt)
+ [Recursive Mirror](./jailbreaks/recursive_mirror_20250620.txt)
+ [Evil Writer](./jailbreaks/evil_writer_20250620.txt)
+ [Sophia](./jailbreaks/sophia_20250620.txt)

#### Prompt Injection

+ [Output All](./prompt_injection/output_all_20250620.txt)

#### System Prompt

+ [Safe Bot](./system_prompt/safe_bot_20250620.txt)

### 2025-06-19

#### Jailbreaks

+ [DAN](./jailbreaks/dan_20250619.txt)

#### Prompt Injection

+ [Label Replacement](./prompt_injection/label_replacement_20250619.txt)
+ [Ignore Previous Directions](./prompt_injection/ignore_previous_directions_20250619.txt)

#### System Prompt

+ [Kali GPT](./system_prompt/kali_gpt_20250619.txt)